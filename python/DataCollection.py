import requests
from bs4 import BeautifulSoup
import re
import oracledb as cx_Oracle
import os
import glob
from pathlib import Path

from rich.console import Console

# âœ… ì •ì±… ìƒì„¸ ì •ë³´ ì„¹ì…˜ í¬ë¡¤ë§
def crawl_all_sections(url):
    response = requests.get(url)    # ìƒì„¸ í˜ì´ì§€ ìš”ì²­
    response.encoding = 'utf-8'     # ì¸ì½”ë”© ì„¤ì •
    soup = BeautifulSoup(response.text, "html.parser")  # HTML íŒŒì‹±

    data_store = {}
    titles = soup.find_all("strong", class_="tit")

    for title in titles:
        section_name = title.get_text(strip=True)   # ì„¹ì…˜ ì œëª© í…ìŠ¤íŠ¸ ì¶”ì¶•
        table = title.find_next("table", class_="form-table form-resp-table")
        if table:
            section_data = {}
            rows = table.find_all("tr") # í‘œì˜ ëª¨ë“  í–‰ì¶”ì¶œ
            for row in rows:
                ths = row.find_all("th")
                tds = row.find_all("td")
                for th, td in zip(ths, tds):
                    key = th.get_text(strip=True)   # í•­ëª© ëª…
                    val = td.get_text(" ", strip=True).replace("\xa0", " ") # ê°’ (ê³µë°± í¬í•¨ ì²˜ë¦¬)
                    section_data[key] = val # ì„¹ì…˜ ë°ì´í„°ì— ì €ì¥
            data_store[section_name] = section_data # ì „ì²´ ì €ì¥ì†Œì— ì„¹ì…˜ ì €ì¥
    return data_store   # ê²°ê³¼ ë°˜í™˜

# âœ… ì§ˆë¬¸ ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
def find_best_section(question, data_store):
    question_lower = question.lower()   # ì§ˆë¬¸ ì†Œë¬¸ì í™”
    best_section = None
    max_matches = 0

    for section_name in data_store.keys():
        matches = sum(1 for word in section_name.lower().split() if word in question_lower)
        # ìœ ì‚¬ ë‹¨ì–´ ìˆ˜ ì²´í¬
        if matches > max_matches:
            max_matches = matches
            best_section = section_name # ê°€ì¥ ìœ ì‚¬í•œ ì„¹ì…˜ ê°±ì‹ 
    return best_section # ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ì„¹ì…˜ ë°˜í™˜

# âœ… ë‹µë³€ ìƒì„±
def generate_answer(question, data_store):
    section = find_best_section(question, data_store)   # ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
    if not section:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    content = data_store.get(section, {})   # ì„¹ì…˜ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    answer = f"{section}\n"
    for k, v in content.items():
        answer += f"{k}: {v}\n" #í•­ëª© : ê°’ í˜•íƒœë¡œ êµ¬ì„±
    return answer   # ì™„ì„±ëœ ë‹µë³€ ë°˜í™˜

# âœ… ì •ì±… ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§
def crawl_policy_list(list_url):
    response = requests.get(list_url)   # ì •ì±… ëª©ë¡ í˜ì´ì§€ ìš”ì²­
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, "html.parser")  #HTML íŒŒì‹±

    policy_items = soup.select("ul.policy-list li") # ì •ì±… í•­ëª©ë“¤ ì„ íƒ
    policy_data = []

    for item in policy_items:
        try:
            category = item.select_one("span.bg-blue").get_text(strip=True) # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            a_tag = item.select_one("a.tit.txt-over1")  # ì œëª©ë§í¬
            title = a_tag.get_text(strip=True)  # ì œëª©ë‚´ìš©
            onclick = a_tag.get("onclick", "")  # í´ë¦­ ì‹œ ì´ë™
            class_attr = a_tag.get("class", []) # í´ë˜ìŠ¤ ì†ì„± ì¶”ì¶œ

            policy_id = ""
            if "goView" in onclick:
                policy_id = onclick.replace("goView('", "").replace("');", "").strip()  # ì •ì±… ID ì¶”ì¶œ

            description = item.select_one("em.txt-over1").get_text(separator=" ", strip=True) # ì„¤ëª… í…ìŠ¤íŠ¸

            policy_data.append({
                "category": category,
                "title": title,
                "policy_id": policy_id,
                "a_class": class_attr,
                "description": description
            })
        except Exception as e:
            print("íŒŒì‹± ì˜¤ë¥˜:", e)
            continue    # ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ í•­ëª© ê±´ë„ˆëœ€
    return policy_data # ìˆ˜ì§‘ëœ ì •ì±… ëª©ë¡ ë°˜í™˜

# âœ… ì •ì±… ID ê¸°ì¤€ ì¤‘ë³µ ì²´í¬
def load_saved_policy_ids_from_files(*file_paths):
    saved_ids = set()   # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ ì§‘í•©
    id_pattern = re.compile(r"plcyBizId=([^&\s]+)") # ì •ì±… ID ì¶”ì¶œ ì •ê·œì‹
    for file_path in file_paths:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    match = id_pattern.search(line) # í•œ ì¤„ì—ì„œ ID ê²€ìƒ‰
                    if match:
                        saved_ids.add(match.group(1).strip())   # IDì €ì¥
        except FileNotFoundError:
            pass # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰

    return saved_ids #ì €ì¥ëœ ID ëª©ë¡ ë°˜í™˜

# âœ… íŠ¹ìˆ˜ë¬¸ì ì œê±°
def remove_special_chars_with_space(text):
    cleaned = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text) # íŠ¹ìˆ˜ë¬¸ì -> ê³µë°±
    cleaned = " ".join(cleaned.split()) # ì—°ì† ê³µë°± ì œê±°

    return cleaned # ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ë°˜í™˜

# âœ… file3 ì €ì¥
#def save_policy_result_to_file(file_path, title, questions, data_store):
def save_policy_result_to_file(file_path, title, questions, data_store, detail_url):
    with open(file_path, "a", encoding="utf-8") as f:
        f.write('"""' + title + "\n") # ì œëª© ì €ì¥
        f.write(detail_url + "\n")  # âœ… URL ì¶”ê°€

        for i, q in enumerate(questions):
            result = (
                generate_answer(q, data_store) # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
                .replace("\n", " ")
                .replace("\xa0", " ")
                .replace("â–¡", "-")
                .replace("ã…‡", "-")
                .replace("Â·", "-")
                .strip()
            )
            result = remove_special_chars_with_space(result) # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            f.write(result + "\n") # ê²°ê³¼ ì €ì¥

        f.write('"""' + "\n") # ë¸”ë¡ ë í‘œì‹œ

# âœ… ì „ì²´ ì •ì±… í˜ì´ì§€ ìˆœíšŒ
def crawl_all_policy_pages():
    all_policies = [] # ëˆ„ì  ì •ì±… ì €ì¥ ë¦¬ìŠ¤íŠ¸
    page = 1 # ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸
    while True:
        list_url = f"https://youth.seoul.go.kr/infoData/plcyInfo/ctList.do?sprtInfoId=&plcyBizId=&key=2309150002&sc_detailAt=&pageIndex={page}&orderBy=regYmd+desc&blueWorksYn=N&tabKind=002&sw=&sc_rcritCurentSitu=001&sc_rcritCurentSitu=002"
        page_policies = crawl_policy_list(list_url) # í•´ë‹¹ í˜ì´ì§€ ì •ì±… ìˆ˜ì§‘
        if not page_policies: # ë”ì´ìƒ ì •ì±… ì—†ìœ¼ë©´ ì¢…ë£Œ
            print("âŒ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
            break
        all_policies.extend(page_policies) # ì •ì±… ëˆ„ì  ì €ì¥
        page += 1 # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
    print(f"\nâœ… ì´ ìˆ˜ì§‘ëœ ì •ì±… ìˆ˜: {len(all_policies)}ê°œ")

    return all_policies # ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼ ë°˜í™˜

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    base_dir = Path(__file__).resolve().parent / "policy_directory/"
    base_file3_name = os.path.join(base_dir, "your_data_file")

    # file3 ê²½ë¡œ íƒìƒ‰
    file3_paths = [
        p for p in glob.glob(os.path.join(base_dir, "your_data_file*.txt"))
    ]

    print("ğŸ“‚ íƒìƒ‰ëœ file ê²½ë¡œë“¤:", file3_paths)

    # file3 ì¸ë±ìŠ¤ ê³„ì‚°
    existing_indexes = []
    for path in file3_paths:
        match = re.search(r"your_data_file(\d+)\.txt", path)
        if match:
            existing_indexes.append(int(match.group(1)))

    file3_index = max(existing_indexes, default=0) + 1
    file3_path = f"{base_file3_name}{file3_index}.txt"
    print(f"ğŸ“ ìµœì´ˆ ì €ì¥ íŒŒì¼: {file3_path}")
    save_count = 0

    # ì¤‘ë³µ ì •ì±… ID ë¡œë”©
    saved_policy_ids = load_saved_policy_ids_from_files(*file3_paths)

    all_policies = crawl_all_policy_pages()

    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    test_questions = [
        "ì‚¬ì—…ê°œìš”ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
        "ì‹ ì²­ìê²©ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì‹ ì²­ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”",
        "ê¸°íƒ€ ì •ë³´ê°€ ìˆë‚˜ìš”?",
        "ì§€ì› ë‚´ìš©ì´ ë­”ê°€ìš”?"
    ]

    dsn = cx_Oracle.makedsn("192.168.0.231", 1521, service_name="helperpdb")
    conn = cx_Oracle.connect(user="helper", password="1111", dsn=dsn)
    cursor = conn.cursor()

    inserted_count = 0

    for i, policy in enumerate(all_policies):
        policy_id = policy["policy_id"]
        if not policy_id:
            continue
        if policy_id in saved_policy_ids:
            print(f"[ì¤‘ë³µ - ID ê¸°ì¤€] '{policy_id}' ì´ë¯¸ ì €ì¥ë˜ì–´ ê±´ë„ˆëœ€")
            continue

        detail_url = f"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId={policy_id}&tab=001&key=2309150002"

        try:
            res = requests.get(detail_url)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, "html.parser")
            policy_title = soup.find("strong", class_="title").get_text(strip=True)
            data_store = crawl_all_sections(detail_url)

            # DB INSERT
            try:
                cursor.execute("INSERT INTO policies (title, url) VALUES (:1, :2)", (policy_title, detail_url))
               # cursor.execute("INSERT INTO policies (title, url) VALUES (:1, :2)", (policy_title, detail_url))
                inserted_count += 1
                print(f"[INSERT ì™„ë£Œ] {policy_title}")
            except cx_Oracle.IntegrityError:
                print(f"[ì¤‘ë³µ - DB ê¸°ì¤€] {policy_title} ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")
            except Exception as e:
                print(f"[DB ERROR] ì œëª© : {policy_title} | ì˜¤ë¥˜ : {e}")

            # file3 ë¶„í•  ì €ì¥
            if save_count >= 20:
                file3_index += 1
                file3_path = f"{base_file3_name}{file3_index}.txt"
                save_count = 0

            save_policy_result_to_file(file3_path, policy_title, test_questions, data_store, detail_url)
            print(f"ğŸ“Œ save_count: {save_count} | í˜„ì¬ íŒŒì¼: {file3_path}")

            save_count += 1
            saved_policy_ids.add(policy_id)

        except Exception as e:
            print(f"[{i+1}] ì •ì±… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - ID: {policy_id} / ì˜¤ë¥˜: {e}")
            continue

    # conn.commit()
    cursor.close()
    conn.close()

    print(f"\nâœ… ìˆ˜ì§‘ëœ ì „ì²´ ì •ì±… ìˆ˜: {len(all_policies)}ê°œ")
    print(f"ğŸŸ¢ DBì— ì‹¤ì œ INSERT ëœ ì •ì±… ìˆ˜: {inserted_count}ê°œ")
    print("\n----------------------- ë°ì´í„° ì €ì¥ ì™„ë£Œ -----------------------------")
